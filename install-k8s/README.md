# Cr√©er un cluster local

Il est tr√®s facile de se cr√©er un cluster de dev avec minikube ou k3s par exemple.  
Mais je trouve √ßa pas mal de s'exercer sur un cluster plus proche de ce que serait un vrai cluster de prod.

Donc l√†, l'objectif c'est de cr√©er un cluster avec un `control-plan` et au moins 1 noeud.

> **_NOTE:_**
> Sur les cluster kubernetes il y a parfois 2 interfaces : 1 interface pour exposer les services et une pour la communication entre les noeuds. L√†, je vais consid√©rer que les VMs sont sur un r√©seau priv√© et qu'il n'est pas utile d'avoir des 2 interfaces.

## Pr√©requis

Le minimum n√©cessaire, c'est :

- Un gestionnaire de machines virtuelles. Ici je vais utiliser [VirtualBox](https://www.virtualbox.org/) mais libre √† vous d'utiliser autre chose si vous √™tes plus √† l'aise avec (QEmu, VMWare, ...).
- `kubectl`. Permettra de contr√¥ler le cluster. Personnellement je l'ai install√© via [asdf](https://asdf-vm.com/).
- `ansible`. Sur mon poste j'ai juste install√© la package `ansible` via apt.

### VM Source

Pour √©viter d'installer X fois des machines, on va installer une VM avec tous les packages requis juste avant de lancer kubeadm.  
Puis on dupliquera ces machines d'abord pour faire le control-plan et ensuite autant de fois que n√©cessaire pour cr√©er le nombre de nodes souhait√©s.

#### Installation de la VM

Je ne vais pas d√©tailler le process.
L'objectif √† la fin c'est d'avoir une VM op√©rationnelle.
Quelques infos quand m√™me :

- Je vous laisse choisir les ressources mais Kubernetes ne s'installera pas avec moins de 2 cpu. Et moins de 4Go suffiraient pour l'installation mais deviendraient peut-√™tre compliqu√©s par la suite.
- C√¥t√© disque, j'ai mis 40Go non pr√©allou√©. L'image finale occupe environ 6Go par noeud.
- La carte doit √™tre en mode `bridge` sinon les VMs ne pourront pas communiquer.
- Je vous conseil d'utiliser La debian stable du moment (12 √† ce jour) pour √™tre au plus compatibel avec la suite.
- Pour faire simple, j'ai laiss√© l'install tout configurer dans une seule partition mais avec LVM quand m√™me au cas ou.
- J'ai mis le clavier en fran√ßais bien sur mais j'ai laiss√© le syst√®me en en-US.
- Au moment de la s√©lection des packages, pas de desktop. Juste `SSH Server` et `Standard system utilities`
- J'ai appel√© cette vm `k8s-src` donc si vous donnez le m√™me nom vosu pourrez copier/coller les commandes suivantes.
- j'ai cr√©√© un utilisateur `k8s`. Si vous l'appelez diff√©remment, il faudra adapter le [playbook](./k8s.common.yml).

Pour continuer v√©rifiez les points suivants :

- vous arrivez √† vous connecter au compte k8s sur la machine sans taper votre mot de passe.
- une fois connect√©, vous arrivez a ex√©cuter la commande `sudo su -` sans avoir √† entrer de mot de passe.

Si c'est le cas, parfait.  
Sinon, je ne vais pas tout d√©tailler non plus mais en gros vous devez, sur la vm :

1. Mettre votre cl√© publique dans `.ssh/authorized` du compte
   ```bash
   $ mkdir .ssh
   $ echo "ssh-ed25519 AAAAC3Nza...wnuSQuWw8c mike@..." >.ssh/authorized_keys
   $ chmod -R go-rwX .ssh/
   ```
2. Installer `sudo`
   ````bash
   $ su -
   $ Password:
   $ root@k8s-src:~# apt install sudo
   ```bash
   ````
3. Enfin, autoriser l'utilisateur k8s a devenir root sans mot de passe.
   On aurait pu utiliser le groupe sudo mais √ßa oblige √† changer la conf par d√©faut.
   Je pr√©f√®re cette m√©thode :
   ```bash
   $ echo "k8s   ALL=(ALL:ALL) NOPASSWD:ALL" >/etc/sudoers.d/k8s
   ```
   Apr√®s √ßa on fait l'install et si on veut revenir √† la conf par d√©faut, il suffit de supprimer le fichier `/etc/sudoers.d/k8s`.
   Mais vu que c'est un cluster de dev, ce ne sera pas n√©cessaire.

### Playbook commun

Pour l'installation des noeuds, les syst√®mes doivent √™tre configur√©s un peu avant. En particulier :

- installation du repo kubernetes
- installation d'un gestionnaire de containers. L√† c'est `containerd`.
- configuration de `containerd` pour que les [control groups](https://fr.wikipedia.org/wiki/Cgroups) soient g√©r√©s par systemd. C'est ainsi que kubernetes pourra contraindre les pods en cpu/memoire/r√©seau/...
- ne pas avoir de swap. Sinon l'initialisation avec kubeadm va fonctionner, mais le cluster ne d√©marrera pas.

C'est ce que fait le playbook [k8s.common.yml](./k8s.common.yml).  
Il s'occupe aussi :

- de d√©sactiver ipv6, pour s'√©viter des probl√®me. On est sur un r√©seau interne donc ce sera plus simple en ipv4.
- installer les packages `kubeadm` et `kubectl`
- t√©l√©charger les images dont aura besoin k8s pour configurer le cluster. Pour √©viter de les re-t√©l√©charger pour chaque noeud.
- installer `jq` qui sera bien pratique pour travailler avec kubernetes.
- installer `rsync ` qui servira √† transf√©rer des fichiers entre notre desktop et les VMs.

Pour l'ex√©cuter, vous avez juste besoin de l'ip de la vm que vous venez de configurer :

```bash
$ ansible-playbook -i 192.168.68.101, k8s.common.yml

PLAY [installation des parties communes] ***************************************************************************************************************************************************************************************************

TASK [Gathering Facts] *********************************************************************************************************************************************************************************************************************
ok: [k8s-src]
...
k8s-src             : ok=19   changed=16   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
```

> **Notez** la virgule apr√®s l'ip dans la commande pr√©c√©dente. Ca permet de faire comprendre √† ansible que c'est le contenu d'un
> inventaire et non le chemin vers un fichier qui contient l'inventaire.

That's it. Si tout se passe bien, on peut passer √† l'installation du controlplan.

### Control plan

On va installer la machine `control-plan` en se basant sur l'image `k8s-src`.

Avec Virtualbox vous pouvez utiliser l'interface, ou vous pouvez utiliser la commande clonevm :

```bash
VBoxManage clonevm  "k8s-src" --basefolder=/home/mike/vms/k8s/ --name k8s-controlplan --register
```

(l'option `basefolder` est facultative, mais j'aime bien mettre toutes les images au m√™me endroit).

si vous utilisez l'interface, faites bien attention √† ce qu'il cr√©e une nouvelle adresse mac pour l'interface r√©seau.  
C'est le cas par d√©faut en ligne de commande.  
(et tant mieux parce que pas envie de chercher comment le param√®trer)

Apr√®s √ßa, vous avez une machine que vous pouvez d√©marrer, √©ventuellement en mode headless :

```bash
$ VBoxManage startvm k8s-controlplan --type headless
Waiting for VM "k8s-controlplan" to power on...
VM "k8s-controlplan" has been successfully started.
```

Le seul int√©r√™t de d√©marrer en mode non headless aurait √©t√© de r√©cup√©rer son ip, mais vous savez que votre dhcp va faire +1 sur l'ip de `k8s-src`. Donc dans mon cas ce sera `192.168.68.99`.

La seule config √† faire sur le serveur est changer son nom.  
Je n'ai pas fait de playbook pour √ßa.
Il suffit de se connecter √† la machine et d'ex√©cuter :

```bash
echo "k8s-controlplan"|sudo tee /etc/hostname
sudo sed -i "s/k8s-src/k8s-controlplan/g" /etc/hosts
```

Il y aura une erreur de sudo mais pas grave.

Toutes les VMs auront les m√™me cl√©s de serveur ssh. Ce n'est pas id√©al mais √ßa ira. Si vous voulez les reg√©n√©rer :

```
rm /etc/ssh/ssh_host_*
dpkg-reconfigure openssh-server
```

Ensuite le mieux est de rebooter (pas n√©cessaire, mais histoire de).

On va maintenant utiliser `kubeadm` pour configurer le cluster.
kubeadm sera utilis√© aussi bien pour configurer le control-plan que pour configurer les nodes.

Pour configurer le control-plan :

```bash
sudo kubeadm init --pod-network-cidr 192.168.71.1/24
```

Le param√®tre `pod-network-cidr` est fonction de votre r√©seau.  
Il sert √† indiquer √† kubernetes sur quelle plage d'adresses il peut cr√©er des services.  
Ce sera utilis√© par le le CNI (Container Network Interface) qu'on installera par la suite.

Voil√† comment j'ai fait pour d√©terminer sa valeur chez moi :  
Ma machine sur le wifi a actuellement l'ip `192.168.68.76/22`. On pourrait calculer le d√©tail du r√©seau,, mais il y a des outils en ligne donc autant les utiliser. En mettant l'ip sur [IP Calculator](https://jodies.de/ipcalc) on voit que ce reseau a 1022 ips allant de `192.168.68.1` a `192.168.71.254`. La plage `192.168.68.1/24` √©tant actuellement utilis√©e par mon dhcp, je vais dire que mon cluster sera sur `192.168.71.1/24` soit 254 adresses entre `192.168.71.1` et `192.168.71.254`.
J'aurais un probl√®me lorsque j'aurais ajout√© environ 400 nouvelles machines sur mon r√©seau, mais on verra bien √† ce moment üôÇ.

Finalement, si tout se passe bien avec `kubeadm` vous devriez avoir une sortie qui ressemble √† √ßa :

```
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.68.104:6443 --token xbr0o0.51l6sf2dajz1ni04 \
        --discovery-token-ca-cert-hash sha256:8aca0cc2bd45d3e460f97888a1bdc1e4c5000c64b21498be6b167ddd7769914f
```

Ex√©cutez tout de suite les commandes qui permettront √† votre utilisateur d'avoir un kubectl correctement configur√© :

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

Apr√®s √ßa, vous devriez pouvoir lancer des commandes kubernetes :

```bash
$ kubectl get node
NAME              STATUS     ROLES           AGE     VERSION
k8s-controlplan   NotReady   control-plane   3m21s   v1.28.5
```

Le status `NotReady`, c'est par ce qu'il reste √† installer un CNI pour g√©rer le r√©seau.

### Installation du CNI

Le CNI s'occupe de g√©rer la partie r√©seau du cluster.  
C'est lui qui va attribuer des ips aux services, configurer les routes, ...

Le plus connu c'est calico :
https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises

L'installation est tr√®s simple puisque le CNI fonctionne lui m√™me comme un `daemon-set` sur le cluster.

Il suffit donc de t√©l√©charger le fichier de d√©finition puis l'installer avec kubectl :

```bash
curl https://raw.githubusercontent.com/projectcalico/calico/v3.26.4/manifests/calico.yaml -O
kubectl apply -f calico.yaml
```

Au bout de quelques secondes, le cluster devrait passer en `Ready` :

```bash
$ kubectl get node
NAME              STATUS   ROLES           AGE   VERSION
k8s-controlplan   Ready    control-plane   96s   v1.28.5
```

### Node 1

L'installation du premier node est une formalit√© √† pr√©sent.

On rep√™te les op√©ration de clone de vm et de configuration effectutu√©e pour le controlplan.

```bash
$ VBoxManage clonevm  "k8s-src" --basefolder=/home/mike/vms/k8s/ --name k8s-node1 --register
0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%
Machine has been successfully cloned as "k8s-node1"

$ VBoxManage startvm k8s-node1 --type headless
Waiting for VM "k8s-node1" to power on...
VM "k8s-node1" has been successfully started.

$ echo "k8s-node1"|sudo tee /etc/hostname
$ sudo sed -i "s/k8s-src/k8s-node1/g" /etc/hosts

$ sudo su -
sudo: unable to resolve host k8s-src: No address associated with hostname
$ rm /etc/ssh/ssh_host_*
$ dpkg-reconfigure openssh-server
Creating SSH2 RSA key; this may take some time ...
3072 SHA256:JdTJcnGJcSbtDFopu+pRAptnEazuCAsewzxZs1ofzfI root@k8s-src (RSA)
...
$ sudo reboot
```

Une fois le node red√©marr√©, on peut utiliser la commande que nous a fournie la sortie de kubeadm sur le control-plan pour configurer le node :

```bash
$ sudo kubeadm join 192.168.68.104:6443 --token xbr0o0.51l6sf2dajz1ni04 \
        --discovery-token-ca-cert-hash sha256:8aca0cc2bd45d3e460f97888a1bdc1e4c5000c64b21498be6b167ddd7769914f
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
...
This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

C'est fini. Sur le control-plan on voir bien le nouveau node :

```bash
$ kubectl get node
NAME              STATUS   ROLES           AGE   VERSION
k8s-controlplan   Ready    control-plane   24m   v1.28.5
k8s-node1         Ready    <none>          18s   v1.28.5
```

On peut r√©peter l'op√©ration autant de fois que n√©cessaire pour ajouter des nodes.
